# file: config.py
"""
Auto-generated skeleton module from api_contract.csv.
Do not edit this file manually; regenerate from the contract instead.
"""

# === Params / Types / Classes ===
from __future__ import annotations

from dataclasses import dataclass, fields
from pathlib import Path
from typing import Any, Dict, Optional

import hashlib
import json
import os
import tempfile


@dataclass
class Config:
    """
    purpose:
        A lightweight, JSON-serializable, project-wide configuration object.

    inputs:
        Instantiated via load_config() from a JSON file; missing fields are filled with defaults.

    outputs:
        A normalized Config with required sections present; unknown keys are preserved.

    failure_modes:
        Raises clear exceptions on invalid JSON, type mismatches, or forbidden token values.

    contract_notes_cn:
        全局配置对象；字段名可在实现中扩展但需保持向后兼容；用于所有模块读取统一口径
        （股票池/窗口集/FAST→FULL门控/缓存预算/并行worker数/keep_last=252等）。

    returns_snapshot:
        Config

    side_effects_snapshot:
        NONE

    version_snapshot:
        v0.1

    parameters_original:
        (dataclass; fields: data_root: str='data', out_root: str='ou...: dict, gp: dict, cache: dict,
        parallel: dict, logging: dict)

    parameters_sanitized:
        fields: data_root: str = 'data', out_root: str = 'o... cache: dict = None, parallel: dict = None,
        logging: dict = None

    sanitization_applied:
        false

    sanitization_reason:

    """

    data_root: str = "data"
    out_root: str = "out"
    state_root: str = "state"

    universe: dict = None
    data: dict = None
    features: dict = None
    ops: dict = None
    expr: dict = None
    fitness: dict = None
    gp: dict = None
    cache: dict = None
    parallel: dict = None
    logging: dict = None

    docs_root: str = "docs"
    run: dict = None
    data_source: dict = None
    sections: dict = None
    meta: dict = None
    paths: dict = None

    def __post_init__(self) -> None:
        """
        Normalize the configuration into a canonical shape.

        - Ensure run/data_source/sections/meta/paths exist and include defaults.
        - Ensure sections contains the required keys and default values.
        - Synchronize legacy per-section fields (universe/ops/expr/fitness/gp/cache/parallel/logging)
          with sections[...] for backward compatibility.
        - Enforce run.keep_last and sections['metrics']['keep_last'] consistency (run is authoritative).
        """

        def _deep_merge(dst: Dict[str, Any], src: Dict[str, Any]) -> Dict[str, Any]:
            for k, v in src.items():
                if isinstance(v, dict) and isinstance(dst.get(k), dict):
                    _deep_merge(dst[k], v)  # type: ignore[index]
                else:
                    dst[k] = v
            return dst

        default_sections: Dict[str, Dict[str, Any]] = {
            "universe": {
                "index_code": "CSI500",
                "drop_st": True,
                "min_list_days": 60,
                "filter_suspended": True,
                "filter_limit_up_down": True,
                "industry_source": "myquant",
                "mcap_source": "myquant",
            },
            "ops": {
                "enabled_groups": ["elementwise", "time_series", "cross_section"],
                "allowed_windows": [3, 5, 10, 20, 60],
                "max_window": 60,
                "allow_ewm": False,
                "cost_model": "simple",
            },
            "expr": {
                "max_nodes": 30,
                "max_depth": 8,
                "allowed_constants": [1, 2, 5, 10],
                "enforce_const_position": True,
                "canonicalize_commutative": True,
            },
            "fitness": {
                "winsor_pct": 0.01,
                "zscore": True,
                "neutralize_industry": True,
                "neutralize_ln_mcap": True,
                "ridge_alpha": 1e-6,
                "ic_method": "rank_ic",
                "fwd_days": 5,
                "ic_agg": "mean",
                "icir_agg": "mean_over_std",
                "min_n": 200,
                "min_days": 40,
                "miss_ratio_max": 0.30,
                "skip_if_insufficient": True,
                "len_penalty": 0.001,
                "max_corr_hard": 0.90,
                "max_corr_soft": 0.70,
                "corr_method": "pearson",
                "corr_scope": "in_gen",
            },
            "fast_full": {
                "fast_days": 60,
                "fast_sample_frac_codes": 0.30,
                "fast_min_n": 120,
                "gate_metric": "ic_mean",
                "gate_ic_mean": 0.01,
                "gate_min_days": 20,
                "gate_max_miss_ratio": 0.35,
                "full_use_all_days": True,
                "full_use_all_codes": True,
            },
            "gp": {
                "rounds": 3,
                "generations": 20,
                "population": 300,
                "tournament_k": 7,
                "elitism": 10,
                "p_crossover": 0.70,
                "p_mutation": 0.30,
                "p_hoist": 0.00,
                "dedup_enable": True,
                "max_expr_len": 30,
                "bloat_penalty": 0.001,
                "checkpoint_every_gen": 1,
                "checkpoint_keep_last": 5,
            },
            "cache": {
                "mem_budget_mb": 2048,
                "disk_budget_gb": 20,
                "enable_disk_cache": True,
                "manifest_dir": "out/cache_plan",
                "materialize": False,
                "invalidate_on_fingerprint_change": True,
            },
            "parallel": {
                "backend": "local_pool",
                "workers": 6,
                "start_method": "spawn",
                "batch_size_expr": 32,
                "max_tasks_in_flight": 128,
            },
            "logging": {
                "level": "INFO",
                "log_dir": "out/logs",
                "console": True,
                "file": True,
                "progress_every": 50,
                "metrics_every_gen": 1,
            },
            "metrics": {
                "ts_dir": "out/ts",
                "keep_last": 252,
                "write_jsonl": True,
                "write_csv": True,
            },
            "rate_limit": {
                "enable": True,
                "qps": 5,
                "burst": 10,
                "backoff": "exponential",
                "max_sleep_s": 60,
            },
        }

        default_run: Dict[str, Any] = {
            "run_id": "default",
            "seed": 42,
            "keep_last": 252,
            "mode": "research",
            "timezone": "Asia/Shanghai",
            "trading_calendar": "CN",
        }

        default_data_source: Dict[str, Any] = {
            "name": "myquant",
            "env": "prod",
            "token_env": "MYQUANT_TOKEN",
            "timeout_s": 30,
            "max_retries": 3,
            "rate_limit_policy": "default",
        }

        default_paths: Dict[str, Any] = {
            "project_root": ".",
            "data_root": self.data_root,
            "out_root": self.out_root,
            "state_root": self.state_root,
            "docs_root": self.docs_root,
            "locks_dir": str(Path(self.state_root) / "locks"),
            "checkpoints_dir": str(Path(self.state_root) / "checkpoints"),
        }

        if self.meta is None:
            self.meta = {}
        if not isinstance(self.meta, dict):
            raise TypeError("Config.meta must be a dict.")

        if self.sections is None:
            self.sections = {}
        if not isinstance(self.sections, dict):
            raise TypeError("Config.sections must be a dict.")

        merged_sections: Dict[str, Dict[str, Any]] = json.loads(json.dumps(default_sections))
        _deep_merge(merged_sections, self.sections)
        self.sections = merged_sections

        if self.run is None:
            self.run = {}
        if not isinstance(self.run, dict):
            raise TypeError("Config.run must be a dict.")

        merged_run: Dict[str, Any] = json.loads(json.dumps(default_run))
        _deep_merge(merged_run, self.run)
        self.run = merged_run

        if self.data_source is None:
            self.data_source = {}
        if not isinstance(self.data_source, dict):
            raise TypeError("Config.data_source must be a dict.")

        merged_ds: Dict[str, Any] = json.loads(json.dumps(default_data_source))
        _deep_merge(merged_ds, self.data_source)
        self.data_source = merged_ds

        for k, v in self.data_source.items(): #禁止在data_source中明文存储token/secret/password
            key_lower = str(k).lower()
            if key_lower == "token_env":
                continue
            if ("token" in key_lower) or ("secret" in key_lower) or ("password" in key_lower):
                if v not in (None, "", False):
                    msg = (
                        "Config.data_source must not contain token/secret/password values: "
                        f"{k} is set."
                    )
                    raise ValueError(msg)

        if self.paths is None:
            self.paths = {}
        if not isinstance(self.paths, dict):
            raise TypeError("Config.paths must be a dict.")

        merged_paths: Dict[str, Any] = json.loads(json.dumps(default_paths))
        _deep_merge(merged_paths, self.paths)
        self.paths = merged_paths

        legacy_map = {
            "universe": "universe",
            "ops": "ops",
            "expr": "expr",
            "fitness": "fitness",
            "gp": "gp",
            "cache": "cache",
            "parallel": "parallel",
            "logging": "logging",
        }
        for legacy_field, sec_key in legacy_map.items():
            legacy_val = getattr(self, legacy_field, None)
            if legacy_val is not None:
                if not isinstance(legacy_val, dict):
                    raise TypeError(f"Config.{legacy_field} must be a dict if provided.")
                _deep_merge(self.sections[sec_key], legacy_val)
            setattr(self, legacy_field, self.sections[sec_key])

        if self.data is None:
            self.data = {}
        if not isinstance(self.data, dict):
            raise TypeError("Config.data must be a dict.")

        if self.features is None:
            self.features = {}
        if not isinstance(self.features, dict):
            raise TypeError("Config.features must be a dict.")

        keep_last_raw = self.run.get("keep_last", 252)
        try:
            keep_last = int(keep_last_raw)
        except Exception as e:
            raise TypeError("Config.run.keep_last must be an int-like value.") from e
        if keep_last <= 0:
            raise ValueError("Config.run.keep_last must be positive.")
        self.run["keep_last"] = keep_last
        self.sections["metrics"]["keep_last"] = keep_last


# === Internal helpers (placeholder) ===
# (Intentionally empty: this generator must not introduce extra helper functions.)


# === Public API (from api_contract.csv) ===

def load_config(path: str) -> Config:
    """
    purpose:
        Load a JSON configuration file and return a normalized Config.

    inputs:
        path: Path to a JSON file (v0.1: JSON only).

    outputs:
        A Config with defaults filled; required section keys are guaranteed to exist.

    failure_modes:
        Raises clear exceptions on missing file, invalid JSON, or type mismatches.

    contract_notes_cn:
        从本地配置文件（建议YAML/JSON）加载Config；实现需做字段校验与默认值填充；
        失败模式：缺失关键字段/类型不匹配需抛明确异常。

    returns_snapshot:
        Config

    side_effects_snapshot:
        READ_DISK

    version_snapshot:
        v0.1

    parameters_original:
        (path: str)

    parameters_sanitized:
        path: str

    sanitization_applied:
        false

    sanitization_reason:

    """
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Config file not found: {p}")

    try:
        raw = p.read_text(encoding="utf-8")
    except Exception as e:
        raise OSError(f"Failed to read config file: {p}") from e

    try:
        obj = json.loads(raw)
    except Exception as e:
        raise ValueError(f"Invalid JSON config: {p}") from e

    if not isinstance(obj, dict):
        msg = f"Top-level config must be a JSON object (dict): {p}"
        raise TypeError(msg)

    known_fields = {f.name for f in fields(Config)}
    extras = {k: v for k, v in obj.items() if (k not in known_fields and k != "meta")}
    kwargs: Dict[str, Any] = {k: v for k, v in obj.items() if k in known_fields}

    meta_val = obj.get("meta")
    if meta_val is None:
        meta_val = {}
    if not isinstance(meta_val, dict):
        raise TypeError(f"Config.meta must be a dict in file: {p}")

    if extras:
        meta_val = dict(meta_val)
        extra_bucket = meta_val.get("_extra")
        if extra_bucket is None:
            meta_val["_extra"] = extras
        else:
            if not isinstance(extra_bucket, dict):
                raise TypeError(f'Config.meta["_extra"] must be a dict in file: {p}')
            extra_bucket.update(extras)

    kwargs["meta"] = meta_val

    if "sections" in kwargs and kwargs["sections"] is not None:
        if not isinstance(kwargs["sections"], dict):
            raise TypeError(f"Config.sections must be a dict in file: {p}")

    cfg = Config(**kwargs)

    required_section_keys = [
        "universe",
        "ops",
        "expr",
        "fitness",
        "fast_full",
        "gp",
        "cache",
        "parallel",
        "logging",
        "metrics",
        "rate_limit",
    ]
    for k in required_section_keys:
        if k not in cfg.sections:
            raise ValueError(f"Missing required sections['{k}'] after normalization: {p}")
        if not isinstance(cfg.sections[k], dict):
            raise TypeError(f"sections['{k}'] must be a dict after normalization: {p}")

    return cfg


def dump_config(cfg: Config, path: str) -> None:
    """
    purpose:
        Serialize Config to JSON using atomic write.

    inputs:
        cfg: Config instance; path: output JSON path.

    outputs:
        None.

    failure_modes:
        Raises clear exceptions on invalid cfg type or write failures.

    contract_notes_cn:
        将Config序列化落盘，供checkpoint引用；要求原子写入，避免中断导致配置损坏。
        [HEURISTIC_FIX_NEEDS_REVIEW]

    returns_snapshot:
        None

    side_effects_snapshot:
        WRITE_DISK

    version_snapshot:
        v0.1

    parameters_original:
        (cfg: Config, path: str)

    parameters_sanitized:
        cfg: Config, path: str

    sanitization_applied:
        false

    sanitization_reason:

    """
    if not isinstance(cfg, Config):
        raise TypeError("cfg must be an instance of Config.")

    out_path = Path(path)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    snapshot: Dict[str, Any] = {
        "data_root": cfg.data_root,
        "out_root": cfg.out_root,
        "state_root": cfg.state_root,
        "docs_root": getattr(cfg, "docs_root", "docs"),
        "paths": cfg.paths,
        "run": cfg.run,
        "data_source": cfg.data_source,
        "sections": cfg.sections,
        "meta": cfg.meta,
        "universe": cfg.universe,
        "data": cfg.data,
        "features": cfg.features,
        "ops": cfg.ops,
        "expr": cfg.expr,
        "fitness": cfg.fitness,
        "gp": cfg.gp,
        "cache": cfg.cache,
        "parallel": cfg.parallel,
        "logging": cfg.logging,
    }

    def _to_jsonable(x: Any) -> Any:
        if isinstance(x, Path):
            return x.as_posix()
        if isinstance(x, dict):
            return {str(k): _to_jsonable(v) for k, v in x.items()}
        if isinstance(x, (list, tuple)):
            return [_to_jsonable(v) for v in x]
        return x

    payload = _to_jsonable(snapshot)

    tmp_name: Optional[str] = None
    try:
        with tempfile.NamedTemporaryFile(
            mode="w",
            encoding="utf-8",
            newline="\n",
            delete=False,
            dir=str(out_path.parent),
            prefix=out_path.name + ".",
            suffix=".tmp",
        ) as f:
            json.dump(payload, f, ensure_ascii=False, indent=2, sort_keys=True)
            f.write("\n")
            tmp_name = f.name
        os.replace(tmp_name, str(out_path))
    except Exception as e:
        try:
            if tmp_name and Path(tmp_name).exists():
                Path(tmp_name).unlink(missing_ok=True)
        except Exception:
            pass
        raise OSError(f"Failed to write config file atomically: {out_path}") from e


def config_fingerprint(cfg: Config) -> str:
    """
    purpose:
        Compute a stable sha256 fingerprint of the configuration.

    inputs:
        cfg: Config instance.

    outputs:
        A hex sha256 string.

    failure_modes:
        Raises clear exceptions if cfg is invalid or not JSON-serializable.

    contract_notes_cn:
        生成配置指纹（如sha256），用于manifest/checkpoint可追溯；指纹变化应触发缓存失效与增量保护。

    returns_snapshot:
        str

    side_effects_snapshot:
        NONE

    version_snapshot:
        v0.1

    parameters_original:
        (cfg: Config)

    parameters_sanitized:
        cfg: Config

    sanitization_applied:
        false

    sanitization_reason:

    """
    if not isinstance(cfg, Config):
        raise TypeError("cfg must be an instance of Config.")

    canonical: Dict[str, Any] = {
        "data_root": cfg.data_root,
        "out_root": cfg.out_root,
        "state_root": cfg.state_root,
        "docs_root": getattr(cfg, "docs_root", "docs"),
        "run": cfg.run,
        "data_source": cfg.data_source,
        "sections": cfg.sections,
        "meta": cfg.meta,
        "paths": cfg.paths,
    }

    path_key_suffixes = ("_root", "_dir", "_path")

    def _normalize(v: Any, key: Optional[str] = None) -> Any:
        if isinstance(v, Path):
            return v.as_posix()
        if isinstance(v, dict):
            items = [(str(k), v[k]) for k in v.keys()]
            items.sort(key=lambda kv: kv[0])
            return {k: _normalize(val, key=k) for k, val in items}
        if isinstance(v, (list, tuple)):
            return [_normalize(x, key=key) for x in v]
        if isinstance(v, str) and key is not None:
            key_lower = key.lower()
            if key_lower.endswith(path_key_suffixes) or ("dir" in key_lower) or ("path" in key_lower):
                return os.path.normpath(v).replace("\\", "/")
        return v

    normalized = _normalize(canonical)
    try:
        blob = json.dumps(
            normalized,
            ensure_ascii=False,
            sort_keys=True,
            separators=(",", ":"),
        )
    except Exception as e:
        raise TypeError("Config contains non-JSON-serializable content for fingerprinting.") from e

    return hashlib.sha256(blob.encode("utf-8")).hexdigest()
